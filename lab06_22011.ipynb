{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praneesha-Daggupati/Praneesha-Daggupati/blob/main/lab05_lab06_22011.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6l0dzmwtd3Ub",
        "outputId": "b2623c93-92f3-4e5c-a717-c9b8aa23cc7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted outputs: [1 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "\n",
        "  def __init__(self, learning_rate=0.1):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Trains the perceptron model on the given data.\n",
        "\n",
        "    Args:\n",
        "      X: A numpy array of shape (n_samples, n_features) representing the training data.\n",
        "      y: A numpy array of shape (n_samples,) representing the target outputs.\n",
        "    \"\"\"\n",
        "    self.weights = np.random.rand(X.shape[1] + 1)  # Add bias term\n",
        "\n",
        "    epochs = 0\n",
        "    while True:\n",
        "      total_error = 0\n",
        "      for i in range(len(X)):\n",
        "        x = X[i]\n",
        "        target_output = y[i]\n",
        "\n",
        "        # Calculate weighted sum\n",
        "        z = np.dot(self.weights[1:], x) + self.weights[0]  # Include bias\n",
        "\n",
        "        # Apply step activation function\n",
        "        predicted_output = 1 if z >= 0 else 0\n",
        "\n",
        "        # Calculate error\n",
        "        error = target_output - predicted_output\n",
        "\n",
        "        # Update weights\n",
        "        self.weights += self.learning_rate * error * np.append(x, 1)  # Include bias update\n",
        "\n",
        "        total_error += abs(error)\n",
        "\n",
        "      epochs += 1\n",
        "      # Stopping criteria: Either low error or maximum epochs reached\n",
        "      if total_error == 0 or epochs > 100:\n",
        "        break\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predicts the output for the given data points.\n",
        "\n",
        "    Args:\n",
        "      X: A numpy array of shape (n_samples, n_features) representing the data points.\n",
        "\n",
        "    Returns:\n",
        "      A numpy array of shape (n_samples,) containing the predicted outputs.\n",
        "    \"\"\"\n",
        "    z = np.dot(self.weights[1:], X.T) + self.weights[0]  # Include bias\n",
        "    return np.where(z >= 0, 1, 0)\n",
        "\n",
        "# Example usage (assuming your data is preprocessed and normalized)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])  # AND gate target outputs\n",
        "\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "predictions = perceptron.predict(X)\n",
        "print(f\"Predicted outputs: {predictions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Sigmoid derivative for updates\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "# Sample inputs (Candies, Mangoes, Milk Packets)\n",
        "X = np.array([\n",
        "    [20, 6, 2],\n",
        "    [16, 3, 6],\n",
        "    [27, 6, 2],\n",
        "    [19, 1, 2],\n",
        "    [24, 4, 2],\n",
        "    [22, 1, 5],\n",
        "    [15, 4, 2],\n",
        "    [18, 4, 2],\n",
        "    [21, 1, 4],\n",
        "    [16, 2, 4]\n",
        "])\n",
        "\n",
        "# Expected outputs, converted Yes/No to 1/0\n",
        "y = np.array([[1, 1, 1, 0, 1, 0, 1, 1, 0, 0]]).T\n",
        "\n",
        "# Initialize weights randomly and bias to zero (for example)\n",
        "np.random.seed(1)  # For consistent results\n",
        "weights = np.random.rand(3, 1)\n",
        "bias = 0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10000):  # Number of iterations\n",
        "    inputs = X\n",
        "    weighted_sum = np.dot(inputs, weights) + bias\n",
        "    outputs = sigmoid(weighted_sum)\n",
        "\n",
        "    # Calculate the error\n",
        "    error = y - outputs\n",
        "\n",
        "    # Adjust weights and bias\n",
        "    adjustments = error * sigmoid_derivative(outputs)\n",
        "    weights += np.dot(inputs.T, adjustments) * learning_rate\n",
        "    bias += np.sum(adjustments) * learning_rate\n",
        "\n",
        "# Display final weights\n",
        "print(\"Weights after training:\")\n",
        "print(weights)\n",
        "print(\"\\nBias after training:\")\n",
        "print(bias)\n"
      ],
      "metadata": {
        "id": "no_9aLcP_Snr",
        "outputId": "fb4deffb-4143-4258-91e4-5867c0475662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights after training:\n",
            "[[-1.44944144]\n",
            " [10.56506654]\n",
            " [-0.43315307]]\n",
            "\n",
            "Bias after training:\n",
            "-0.8912290898681886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Input datasets\n",
        "inputs = np.array([[0,0],\n",
        "                   [0,1],\n",
        "                   [1,0],\n",
        "                   [1,1]])\n",
        "expected_output = np.array([[0],[0],[0],[1]])\n",
        "\n",
        "epochs = 10000\n",
        "learning_rate = 0.05\n",
        "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2, 2, 1\n",
        "\n",
        "# Random weights and bias initialization\n",
        "hidden_weights = np.random.uniform(size=(inputLayerNeurons, hiddenLayerNeurons))\n",
        "hidden_bias = np.random.uniform(size=(1, hiddenLayerNeurons))\n",
        "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
        "output_bias = np.random.uniform(size=(1,outputLayerNeurons))\n",
        "\n",
        "# Training algorithm\n",
        "for _ in range(epochs):\n",
        "    # Forward Propagation\n",
        "    hidden_layer_activation = np.dot(inputs, hidden_weights)\n",
        "    hidden_layer_activation += hidden_bias\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    output_layer_activation = np.dot(hidden_layer_output, output_weights)\n",
        "    output_layer_activation += output_bias\n",
        "    predicted_output = sigmoid(output_layer_activation)\n",
        "\n",
        "    # Backpropagation\n",
        "    error = expected_output - predicted_output\n",
        "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Updating Weights and Biases\n",
        "    output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * learning_rate\n",
        "    hidden_weights += inputs.T.dot(d_hidden_layer) * learning_rate\n",
        "    hidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * learning_rate\n",
        "\n",
        "print(\"Final hidden weights: \", hidden_weights)\n",
        "print(\"Final hidden bias: \", hidden_bias)\n",
        "print(\"Final output weights: \", output_weights)\n",
        "print(\"Final output bias: \", output_bias)\n",
        "print(\"\\nOutput from neural network after 10,000 epochs: \",predicted_output)\n"
      ],
      "metadata": {
        "id": "r215yQF0AbL_",
        "outputId": "7e1e40c5-6ad0-4ae4-8c56-e6ebc2f77ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final hidden weights:  [[ 1.19942422 -3.24144432]\n",
            " [ 0.91942564 -3.337107  ]]\n",
            "Final hidden bias:  [[-0.38218905  4.55818858]]\n",
            "Final output weights:  [[ 2.61097548]\n",
            " [-7.37581774]]\n",
            "Final output bias:  [[1.10443892]]\n",
            "\n",
            "Output from neural network after 10,000 epochs:  [[0.00584833]\n",
            " [0.05004472]\n",
            " [0.05209307]\n",
            " [0.9213462 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to train and evaluate MLPClassifier\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, problem_description):\n",
        "    # Initialize MLPClassifier\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', max_iter=1000, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = mlp.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {problem_description}: {accuracy}\")\n",
        "\n",
        "# Load your project dataset\n",
        "# Replace 'your_dataset.csv' with the actual file path or URL\n",
        "file_path = '/content/drive/MyDrive/Bangli-P10_gabor.csv'\n",
        "data_project = pd.read_csv(file_path)\n",
        "\n",
        "# Extract features and target from the project dataset\n",
        "X_project = data_project.iloc[:, :-1].values\n",
        "y_project = data_project.iloc[:, -1].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_project, X_test_project, y_train_project, y_test_project = train_test_split(\n",
        "    X_project, y_project, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# A10: MLP for AND gate logic\n",
        "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([[1, 0], [1, 0], [1, 0], [0, 1]])\n",
        "\n",
        "train_and_evaluate(X_and, y_and, X_and, y_and, \"AND Gate Logic with MLP\")\n",
        "\n",
        "# A10: MLP for XOR gate logic\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n",
        "\n",
        "train_and_evaluate(X_xor, y_xor, X_xor, y_xor, \"XOR Gate Logic with MLP\")\n",
        "\n",
        "# A11: MLP for project dataset\n",
        "# Assuming the last column in your project dataset is the target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_project, y_project, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_and_evaluate(X_train, y_train, X_test, y_test, \"Project Dataset with MLP\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yFhBa-JnAz8B",
        "outputId": "fb6c40e2-4419-49f8-e1da-d86f5c0871eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for AND Gate Logic with MLP: 0.75\n",
            "Accuracy for XOR Gate Logic with MLP: 0.5\n"
          ]
        }
      ]
    }
  ]
}
