{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KxARqEnTfEwLNQGFoMh2DNE8u1KTY46p",
      "authorship_tag": "ABX9TyNgNn6YYCYtkR18fdmn58L0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praneesha-Daggupati/Praneesha-Daggupati/blob/main/lab_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQxmAXmC8onk",
        "outputId": "b074060f-4d20-4351-fda7-45173394368d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Node Feature: Gabor2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_entropy(y):\n",
        "    \"\"\"\n",
        "    Calculate entropy for a given target variable.\n",
        "    \"\"\"\n",
        "    classes = np.unique(y)\n",
        "    entropy = 0\n",
        "    total_samples = len(y)\n",
        "    for cls in classes:\n",
        "        p_cls = np.sum(y == cls) / total_samples\n",
        "        entropy -= p_cls * np.log2(p_cls)\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(X, y, feature):\n",
        "    \"\"\"\n",
        "    Calculate information gain for a given feature.\n",
        "    \"\"\"\n",
        "    # Calculate entropy of the entire dataset\n",
        "    total_entropy = calculate_entropy(y)\n",
        "\n",
        "    # Calculate entropy for each unique value of the feature\n",
        "    unique_values = np.unique(X[feature])\n",
        "    entropy_feature = 0\n",
        "    total_samples = len(y)\n",
        "    for value in unique_values:\n",
        "        subset_indices = X[feature] == value\n",
        "        subset_entropy = calculate_entropy(y[subset_indices])\n",
        "        entropy_feature += (np.sum(subset_indices) / total_samples) * subset_entropy\n",
        "\n",
        "    # Calculate information gain\n",
        "    information_gain = total_entropy - entropy_feature\n",
        "    return information_gain\n",
        "\n",
        "def find_root_node(X, y):\n",
        "    \"\"\"\n",
        "    Find the root node feature based on information gain.\n",
        "    \"\"\"\n",
        "    features = X.columns\n",
        "    max_information_gain = -1\n",
        "    best_feature = None\n",
        "    for feature in features:\n",
        "        information_gain = calculate_information_gain(X, y, feature)\n",
        "        if information_gain > max_information_gain:\n",
        "            max_information_gain = information_gain\n",
        "            best_feature = feature\n",
        "    return best_feature\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming data is loaded from a CSV file\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/Bangli-P10_gabor.csv\", nrows=1048571)\n",
        "\n",
        "    # Assuming the second column is the target variable\n",
        "    y = data.iloc[:, 2]     # Target\n",
        "    X = data.iloc[:, 3:30]  # Features\n",
        "\n",
        "    # Find the root node feature\n",
        "    root_node = find_root_node(X, y)\n",
        "    print(\"Root Node Feature:\", root_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_entropy(y):\n",
        "    \"\"\"\n",
        "    Calculate entropy for a given target variable.\n",
        "    \"\"\"\n",
        "    classes = np.unique(y)\n",
        "    entropy = 0\n",
        "    total_samples = len(y)\n",
        "    for cls in classes:\n",
        "        p_cls = np.sum(y == cls) / total_samples\n",
        "        entropy -= p_cls * np.log2(p_cls)\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(X, y, feature):\n",
        "    \"\"\"\n",
        "    Calculate information gain for a given feature.\n",
        "    \"\"\"\n",
        "    # Calculate entropy of the entire dataset\n",
        "    total_entropy = calculate_entropy(y)\n",
        "\n",
        "    # Calculate entropy for each unique value of the feature\n",
        "    unique_values = np.unique(X[feature])\n",
        "    entropy_feature = 0\n",
        "    total_samples = len(y)\n",
        "    for value in unique_values:\n",
        "        subset_indices = X[feature] == value\n",
        "        subset_entropy = calculate_entropy(y[subset_indices])\n",
        "        entropy_feature += (np.sum(subset_indices) / total_samples) * subset_entropy\n",
        "\n",
        "    # Calculate information gain\n",
        "    information_gain = total_entropy - entropy_feature\n",
        "    return information_gain\n",
        "\n",
        "def find_root_node(X, y):\n",
        "    \"\"\"\n",
        "    Find the root node feature based on information gain.\n",
        "    \"\"\"\n",
        "    features = X.columns\n",
        "    max_information_gain = -1\n",
        "    best_feature = None\n",
        "    for feature in features:\n",
        "        information_gain = calculate_information_gain(X, y, feature)\n",
        "        if information_gain > max_information_gain:\n",
        "            max_information_gain = information_gain\n",
        "            best_feature = feature\n",
        "    return best_feature\n",
        "\n",
        "def equal_width_binning(feature, num_bins):\n",
        "    \"\"\"\n",
        "    Perform equal width binning for a continuous feature.\n",
        "    \"\"\"\n",
        "    bins = pd.cut(feature, bins=num_bins, labels=False)\n",
        "    return bins\n",
        "\n",
        "def frequency_binning(feature, num_bins):\n",
        "    \"\"\"\n",
        "    Perform frequency binning for a continuous feature.\n",
        "    \"\"\"\n",
        "    bins = pd.qcut(feature, q=num_bins, labels=False, duplicates='drop')\n",
        "    return bins\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming data is loaded from a CSV file\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/Bangli-P10_gabor.csv\", nrows=1048571)\n",
        "\n",
        "    # Assuming the second column is the target variable\n",
        "    y = data.iloc[:, 2]     # Target\n",
        "    X = data.iloc[:, 3:28].copy()  # Features (make a copy to avoid SettingWithCopyWarning)\n",
        "\n",
        "    # Binning the continuous features\n",
        "    for column in X.columns:\n",
        "        if X[column].dtype == 'float64' or X[column].dtype == 'int64':\n",
        "            # Using equal width binning with 5 bins as default\n",
        "            X[column] = equal_width_binning(X[column], num_bins=5)\n",
        "\n",
        "    # Find the root node feature\n",
        "    root_node = find_root_node(X, y)\n",
        "    print(\"Root Node Feature:\", root_node)\n",
        "\n",
        "    # Display the binned features\n",
        "    print(\"\\nBinned Features:\")\n",
        "    print(X.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VW6KYmrCj1y",
        "outputId": "d8c0668a-c8f8-4ba8-8e88-b2b167021a3c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Node Feature: Gabor2\n",
            "\n",
            "Binned Features:\n",
            "   Gabor2  Gabor3  Gabor4  Gabor5  Gabor6  Gabor7  Gabor8  Gabor9  Gabor10  \\\n",
            "0       2       4       4       4       4       4       4       2        2   \n",
            "1       2       4       4       4       4       4       4       2        2   \n",
            "2       2       4       4       4       4       4       4       2        2   \n",
            "3       2       4       4       4       4       4       4       2        2   \n",
            "4       2       4       4       4       4       4       4       2        2   \n",
            "\n",
            "   Gabor11  ...  Gabor17  Gabor18  Gabor19  Gabor20  Gabor21  Gabor22  \\\n",
            "0        4  ...        2        2        2        2        4        4   \n",
            "1        4  ...        2        2        2        2        4        4   \n",
            "2        4  ...        2        2        2        2        4        4   \n",
            "3        4  ...        2        2        2        2        4        4   \n",
            "4        4  ...        2        2        2        2        4        4   \n",
            "\n",
            "   Gabor23  Gabor24  Gabor25  Gabor26  \n",
            "0        4        4        2        2  \n",
            "1        4        4        2        2  \n",
            "2        4        4        2        2  \n",
            "3        4        4        2        2  \n",
            "4        4        4        2        2  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def calculate_entropy(self, y):\n",
        "        \"\"\"\n",
        "        Calculate entropy for a given target variable.\n",
        "        \"\"\"\n",
        "        classes = np.unique(y)\n",
        "        entropy = 0\n",
        "        total_samples = len(y)\n",
        "        for cls in classes:\n",
        "            p_cls = np.sum(y == cls) / total_samples\n",
        "            entropy -= p_cls * np.log2(p_cls)\n",
        "        return entropy\n",
        "\n",
        "    def calculate_information_gain(self, X, y, feature):\n",
        "        \"\"\"\n",
        "        Calculate information gain for a given feature.\n",
        "        \"\"\"\n",
        "        # Calculate entropy of the entire dataset\n",
        "        total_entropy = self.calculate_entropy(y)\n",
        "\n",
        "        # Calculate entropy for each unique value of the feature\n",
        "        unique_values = np.unique(X[feature])\n",
        "        entropy_feature = 0\n",
        "        total_samples = len(y)\n",
        "        for value in unique_values:\n",
        "            subset_indices = X[feature] == value\n",
        "            subset_entropy = self.calculate_entropy(y[subset_indices])\n",
        "            entropy_feature += (np.sum(subset_indices) / total_samples) * subset_entropy\n",
        "\n",
        "        # Calculate information gain\n",
        "        information_gain = total_entropy - entropy_feature\n",
        "        return information_gain\n",
        "\n",
        "    def find_best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature to split on based on information gain.\n",
        "        \"\"\"\n",
        "        features = X.columns\n",
        "        max_information_gain = -1\n",
        "        best_feature = None\n",
        "        for feature in features:\n",
        "            information_gain = self.calculate_information_gain(X, y, feature)\n",
        "            if information_gain > max_information_gain:\n",
        "                max_information_gain = information_gain\n",
        "                best_feature = feature\n",
        "        return best_feature\n",
        "\n",
        "    def split_dataset(self, X, y, feature, value):\n",
        "        \"\"\"\n",
        "        Split the dataset based on the chosen feature and its value.\n",
        "        \"\"\"\n",
        "        left_indices = X[feature] == value\n",
        "        right_indices = ~left_indices\n",
        "        X_left, y_left = X[left_indices], y[left_indices]\n",
        "        X_right, y_right = X[right_indices], y[right_indices]\n",
        "        return X_left, y_left, X_right, y_right\n",
        "\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively build the Decision Tree.\n",
        "        \"\"\"\n",
        "        # Check for stopping criteria\n",
        "        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        # Find the best feature to split on\n",
        "        best_feature = self.find_best_split(X, y)\n",
        "\n",
        "        # Split the dataset based on the best feature\n",
        "        unique_values = np.unique(X[best_feature])\n",
        "        node = {best_feature: {}}\n",
        "        for value in unique_values:\n",
        "            X_left, y_left, X_right, y_right = self.split_dataset(X, y, best_feature, value)\n",
        "            node[best_feature][value] = self.build_tree(X_left, y_left, depth + 1), self.build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def predict(self, tree, sample):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained Decision Tree.\n",
        "        \"\"\"\n",
        "        if not isinstance(tree, dict):\n",
        "            return tree\n",
        "\n",
        "        feature = list(tree.keys())[0]\n",
        "        value = sample[feature]\n",
        "        if value not in tree[feature]:\n",
        "            return None\n",
        "\n",
        "        sub_tree = tree[feature][value]\n",
        "        return self.predict(sub_tree, sample)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming data is loaded from a CSV file\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/Bangli-P10_gabor.csv\", nrows=400)\n",
        "\n",
        "    # Assuming the second column is the target variable\n",
        "    y = data.iloc[:, 1]     # Target\n",
        "    X = data.iloc[:, 2:32]  # Features\n",
        "\n",
        "    # Create an instance of DecisionTree\n",
        "    dt = DecisionTree(max_depth=3)\n",
        "\n",
        "    # Build the Decision Tree\n",
        "    tree = dt.build_tree(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    sample = X.iloc[0]\n",
        "    prediction = dt.predict(tree, sample)\n",
        "    print(\"Prediction for sample:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Ja0MDHDdDU",
        "outputId": "80df8bdf-77b4-40fd-c7d3-8c49143386d6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for sample: ({'Gabor6': {206: ({'Gabor8': {83: (226, 227), 84: (227, 226)}}, {'Gabor21': {134: (228, 227), 135: (227, 227), 136: (227, 227), 137: (227, 227)}}), 207: ({'Gabor21': {134: (228, 227), 135: (227, 227), 136: (227, 227), 137: (227, 227)}}, {'Gabor8': {83: (226, 227), 84: (227, 226)}}), 208: ({'Gabor7': {153: (227, 227), 154: (227, 227)}}, {'Gabor6': {206: (226, 227), 207: (227, 226)}})}}, {'Gabor6': {206: (228, {'Gabor6': {207: (228, 228), 208: (228, 228)}}), 207: ({'Gabor29': {105: (228, 228), 106: (228, 228), 107: (228, 228)}}, {'Gabor7': {152: (228, 228), 153: (228, 229), 154: (229, 228)}}), 208: ({'Gabor7': {153: (228, 229), 154: (229, 228)}}, {'Gabor29': {105: (228, 228), 106: (228, 228), 107: (228, 228)}})}})\n"
          ]
        }
      ]
    }
  ]
}